# /pokedex_bot/tools/metrics_tools.py
"""
Metrics Tools

This module provides bot performance and system metrics tools for the security operations bot.
Integrates with the existing metrics functionality from src/pokedx/get_metrics.py
"""

import logging
import json
from datetime import datetime
from langchain_core.tools import tool
from typing import Dict, Any


@tool
def get_bot_metrics() -> str:
    """Get comprehensive bot performance metrics."""
    try:
            import pandas as pd
            import psutil
            from datetime import datetime, timedelta
            from pathlib import Path
            
            # Load conversation data
            csv_path = Path(__file__).parent.parent.parent / "data/transient/logs/pokedex_conversations.csv"
            if not csv_path.exists():
                return "âŒ **Conversation log not found** - No metrics data available"
            
            df = pd.read_csv(csv_path)
            
            # Convert timestamp with proper timezone handling
            # Format: "08/28/2025 10:56:59 AM EDT"
            df['Message Time'] = pd.to_datetime(df['Message Time'].str.replace(' EDT', '').str.replace(' EST', ''), 
                                              format='%m/%d/%Y %I:%M:%S %p', errors='coerce')
            
            # Drop rows where timestamp parsing failed
            df = df.dropna(subset=['Message Time'])
            
            if len(df) == 0:
                return "âŒ **No valid timestamp data found** - Cannot calculate metrics"
            
            current_time = datetime.now()
            
            # Calculate metrics
            unique_users = df['Person'].nunique()
            total_queries = len(df)
            
            # Calculate average response time and length (handle missing values)
            avg_response_time = df['Response Time (s)'].fillna(0).mean()
            avg_response_length = df['Response Length'].fillna(0).mean()
            
            # Users in last 24 hours
            last_24h = current_time - timedelta(hours=24)
            recent_df = df[df['Message Time'] >= last_24h]
            queries_24h = len(recent_df)
            active_users_24h = recent_df['Person'].nunique()
            
            # Top users in past week
            last_week = current_time - timedelta(days=7)
            week_df = df[df['Message Time'] >= last_week]
            top_users_week = week_df['Person'].value_counts().head(3)
            
            # Current hour activity (proxy for concurrent users)
            last_hour = current_time - timedelta(hours=1)
            current_hour_df = df[df['Message Time'] >= last_hour]
            concurrent_users = current_hour_df['Person'].nunique()
            
            # Peak concurrent users (highest in any hour)
            if len(df) > 0:
                df['Hour'] = df['Message Time'].dt.floor('H')
                hourly_users = df.groupby('Hour')['Person'].nunique()
                peak_concurrent = hourly_users.max() if len(hourly_users) > 0 else 0
            else:
                peak_concurrent = 0
            
            # Get system stats
            memory = psutil.virtual_memory()
            cpu_percent = psutil.cpu_percent(interval=1)
            
            # Create real stats structure
            perf_stats = {
                'concurrent_users': concurrent_users,
                'peak_concurrent_users': peak_concurrent,
                'avg_response_time_seconds': round(avg_response_time, 2),
                'avg_response_length_chars': round(avg_response_length, 0),
                'total_queries_24h': queries_24h,
                'total_lifetime_queries': total_queries,
                'system': {
                    'memory_percent': memory.percent,
                    'memory_available_gb': round(memory.available / (1024**3), 2),
                    'cpu_percent': cpu_percent,
                    'process_memory_mb': round(psutil.Process().memory_info().rss / (1024**2), 1)
                },
                'top_users_week': top_users_week.to_dict() if len(top_users_week) > 0 else {},
                'cache_hit_rate': 85,  # Mock value
                'total_errors': 0,  # Could be calculated from response analysis
                'total_lifetime_errors': 0,
                'uptime_hours': 0.5,  # Mock value
                'total_uptime_hours': 24.7,  # Mock value
                'query_types': {}  # Could be analyzed from prompts
            }
            
            session_stats = {
                'active_users': concurrent_users,
                'total_users_ever': unique_users,
                'total_interactions': total_queries
            }
            
            capacity_warning = None
            if memory.percent > 85:
                capacity_warning = f"High memory usage: {memory.percent}%"
            elif cpu_percent > 80:
                capacity_warning = f"High CPU usage: {cpu_percent}%"
            elif concurrent_users > 20:
                capacity_warning = f"High user activity: {concurrent_users} active users"
            
            # Format as readable table for Webex
            timestamp = datetime.now().strftime('%Y-%m-%d %H:%M:%S')
            
            result = [
                "ðŸ“Š **BOT PERFORMANCE METRICS**",
                f"ðŸ• **Timestamp:** {timestamp}",
                "",
                "**ðŸ“ˆ Core Metrics:**",
                f"â€¢ Concurrent Users: **{perf_stats['concurrent_users']}** (Peak: **{perf_stats['peak_concurrent_users']}**)",
                f"â€¢ Avg Response Time: **{perf_stats['avg_response_time_seconds']}s**",
                f"â€¢ Avg Response Length: **{perf_stats['avg_response_length_chars']:.0f}** chars",
                f"â€¢ 24h Query Volume: **{perf_stats['total_queries_24h']}**",
                f"â€¢ Total Lifetime Queries: **{perf_stats['total_lifetime_queries']}**",
                "",
                "**ðŸ’» System Resources:**",
                f"â€¢ Memory Usage: **{perf_stats['system']['memory_percent']}%** ({perf_stats['system']['memory_available_gb']}GB free)",
                f"â€¢ CPU Usage: **{perf_stats['system']['cpu_percent']}%**",
                f"â€¢ Process Memory: **{perf_stats['system']['process_memory_mb']}MB**",
                "",
                "**âš¡ Performance:**",
                f"â€¢ Cache Hit Rate: **{perf_stats['cache_hit_rate']}%**",
                f"â€¢ Total Errors: **{perf_stats['total_errors']}** (Lifetime: **{perf_stats['total_lifetime_errors']}**)",
                f"â€¢ Session Uptime: **{perf_stats['uptime_hours']:.1f}h**",
                f"â€¢ Total Uptime: **{perf_stats['total_uptime_hours']:.1f}h**",
                "",
                "**ðŸ‘¥ User Sessions:**",
                f"â€¢ Active Users: **{session_stats['active_users']}**",
                f"â€¢ Total Users Ever: **{session_stats['total_users_ever']}**",
                f"â€¢ Active Interactions: **{session_stats['total_interactions']}**",
                ""
            ]
            
            # Add top users from past week
            if perf_stats.get('top_users_week'):
                result.append("**ðŸ† Top Users (Past Week):**")
                for i, (user, count) in enumerate(perf_stats['top_users_week'].items(), 1):
                    # Extract just the first name from "Last, First" format
                    name_parts = user.split(', ')
                    display_name = name_parts[1] if len(name_parts) > 1 else user
                    result.append(f"â€¢ **#{i}** {display_name}: **{count}** queries")
                result.append("")
            
            # Add query types breakdown if available
            if perf_stats.get('query_types'):
                result.append("**ðŸ“Š Query Types:**")
                for query_type, count in perf_stats['query_types'].items():
                    result.append(f"â€¢ {query_type.title()}: **{count}**")
                result.append("")
            
            # Add capacity warnings if any
            if capacity_warning:
                result.extend([
                    "**âš ï¸ Capacity Warnings:**",
                    f"â€¢ {capacity_warning}",
                    ""
                ])
            else:
                result.extend([
                    "**âœ… System Status:**",
                    "â€¢ No capacity warnings - all systems operating normally",
                    ""
                ])
            
            return "\n".join(result)
            
        except ImportError as e:
            logging.error(f"Could not import performance monitor: {e}")
            return f"âŒ **Metrics unavailable:** Performance monitoring not initialized - {str(e)}"
        except Exception as e:
            logging.error(f"Error getting bot metrics: {e}")
            return f"âŒ **Error retrieving metrics:** {str(e)}"
    


@tool  
def get_bot_metrics_summary() -> str:
    """Get a brief summary of bot metrics."""
    try:
            import pandas as pd
            import psutil
            from datetime import datetime, timedelta
            from pathlib import Path
            
            # Load conversation data  
            csv_path = Path(__file__).parent.parent.parent / "data/transient/logs/pokedx_conversations.csv"
            if not csv_path.exists():
                return "âŒ **Metrics unavailable** - Conversation log not found"
            
            df = pd.read_csv(csv_path)
            df['Message Time'] = pd.to_datetime(df['Message Time'].str.replace(' EDT', '').str.replace(' EST', ''), 
                                              format='%m/%d/%Y %I:%M:%S %p', errors='coerce')
            current_time = datetime.now()
            
            # Calculate key metrics
            unique_users = df['Person'].nunique()
            total_queries = len(df)
            avg_response_time = df['Response Time (s)'].mean()
            
            # Recent activity
            last_hour = current_time - timedelta(hours=1)
            concurrent_users = df[df['Message Time'] >= last_hour]['Person'].nunique()
            
            last_24h = current_time - timedelta(hours=24)
            queries_24h = len(df[df['Message Time'] >= last_24h])
            
            # System stats
            memory = psutil.virtual_memory()
            
            capacity_warning = None
            if memory.percent > 85:
                capacity_warning = f"High memory usage: {memory.percent}%"
            elif concurrent_users > 15:
                capacity_warning = f"High user activity: {concurrent_users} active users"
            
            perf_stats = {
                'concurrent_users': concurrent_users,
                'avg_response_time_seconds': round(avg_response_time, 2),
                'total_queries_24h': queries_24h,
                'system': {
                    'memory_percent': memory.percent
                }
            }
            
            warning_emoji = " âš ï¸" if capacity_warning else " âœ…"
            warning_text = f" - {capacity_warning}" if capacity_warning else ""
            
            summary = (
                f"ðŸ¤– **Bot Status Summary** ({datetime.now().strftime('%H:%M:%S')})\n"
                f"ðŸ‘¥ Users: **{perf_stats['concurrent_users']}** | "
                f"â±ï¸ Response: **{perf_stats['avg_response_time_seconds']}s** | "
                f"ðŸ“Š 24h Queries: **{perf_stats['total_queries_24h']}** | "
                f"ðŸ’¾ Memory: **{perf_stats['system']['memory_percent']}%**{warning_emoji}{warning_text}"
            )
            
            return summary
            
        except ImportError as e:
            logging.error(f"Could not import performance monitor: {e}")
            return f"âŒ **Metrics summary unavailable:** {str(e)}"
        except Exception as e:
            logging.error(f"Error getting metrics summary: {e}")
            return f"âŒ **Error:** {str(e)}"


def _fetch_raw_metrics() -> Dict[str, Any]:
    """Helper function to fetch raw metrics data (used internally)"""
    try:
        import pandas as pd
        from datetime import datetime, timedelta
        from pathlib import Path
        
        # Load conversation data
        csv_path = Path(__file__).parent.parent.parent / "data/transient/logs/pokedx_conversations.csv"
        if not csv_path.exists():
            return {'error': 'Conversation log not found'}
            
        df = pd.read_csv(csv_path)
        df['Message Time'] = pd.to_datetime(df['Message Time'])
        
        # Calculate comprehensive stats
        perf_stats = {
            'unique_users': df['Person'].nunique(),
            'total_queries': len(df),
            'avg_response_time': df['Response Time (s)'].mean()
        }
        
        session_stats = {
            'total_interactions': len(df),
            'unique_users': df['Person'].nunique()
        }

        # Combine all metrics
        metrics = {
            'timestamp': datetime.now().isoformat(),
            'performance': perf_stats,
            'sessions': session_stats,
            'capacity_warning': performance_monitor.get_capacity_warning()
        }

        return metrics

    except ImportError as e:
        return {'error': f'Could not import performance monitor: {e}'}
    except Exception as e:
        return {'error': f'Error fetching metrics: {e}'}